{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting to Philosophy_WebScraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clicking on the first link in the main text of a Wikipedia article, and then repeating the process for subsequent articles, usually leads to the Philosophy article.**\n",
    "\n",
    "**The program should receive a Wikipedia link as an input, go to another normal link and repeat this process until either Philosophy page is reached, or we are in an article without any outgoing Wikilinks, or stuck in a loop.and finally print all visited links to the standard output.**\n",
    "\n",
    "**we will use https://en.wikipedia.org/wiki/Special:Random to check this hypothesis at home.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the used libraries \n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import time\n",
    "import sys\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start\n",
    "start_url = \"https://en.wikipedia.org/wiki/Special:Random\"\n",
    "# target\n",
    "target_url = \"https://en.wikipedia.org/wiki/Philosophy\"\n",
    "# store the visited article \n",
    "visited_urls = [start_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_first_link(url):\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # This div stars with the body of the article\n",
    "    content_div = soup.find(id=\"mw-content-text\").find(class_=\"mw-parser-output\")\n",
    "\n",
    "    # if the link contains no links it remains None\n",
    "    article_link = None\n",
    "\n",
    "    # Find all the direct children of content_div that are paragraphs\n",
    "    for element in content_div.find_all(\"p\", recursive=False):\n",
    "        #find only the direct children\n",
    "        if element.find(\"a\", recursive=False):\n",
    "            article_link = element.find(\"a\", recursive=False).get('href')\n",
    "            break\n",
    "\n",
    "    if not article_link:\n",
    "        return\n",
    "\n",
    "    # Build a full url \n",
    "    first_link = urllib.parse.urljoin(\n",
    "        'https://en.wikipedia.org/', article_link)\n",
    "\n",
    "    return first_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_scraping(scraping_history, target_url, max_steps=30):\n",
    "    # When reaches to philosphy\n",
    "    if scraping_history[-1] == target_url:\n",
    "        print(\"Target ('Philosphy') article reached!\")\n",
    "        return False\n",
    "    # max iterations \n",
    "    elif len(scraping_history) > max_steps:\n",
    "        print(\"Maximum (30) searches reached, interrupted.\")\n",
    "        return False\n",
    "    elif scraping_history[-1] in scraping_history[:-1]:\n",
    "        print(\"We are in a Loop , interrupted.\")\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Special:Random\n",
      "https://en.wikipedia.org/wiki/Conservatism_in_the_United_States\n",
      "https://en.wikipedia.org/wiki/Political_philosophy\n",
      "https://en.wikipedia.org/wiki/Politics\n",
      "https://en.wikipedia.org/wiki/Greek_language\n",
      "Arrived at an article with no links, search aborted.\n"
     ]
    }
   ],
   "source": [
    "while continue_scraping(visited_urls, target_url):\n",
    "    #print first link\n",
    "    print(visited_urls[-1])\n",
    "\n",
    "    first_link = find_first_link(visited_urls[-1])\n",
    "    # when arrive at an article with no links\n",
    "    if not first_link:\n",
    "        print(\"Arrived at an article with no links, search aborted.\")\n",
    "        break\n",
    "\n",
    "    visited_urls.append(first_link)\n",
    "\n",
    "    time.sleep(0.5)  # Slow things down so as to not overload Wikipedia's servers\n",
    "visited_urls=[start_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
